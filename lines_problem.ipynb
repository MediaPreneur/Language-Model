{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import os\n",
    "import librosa as lr\n",
    "import shutil\n",
    "\n",
    "from skimage.io import imread\n",
    "import h5py\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dim = (192,192,1)\n",
    "out_dim = 176\n",
    "batch_size = 32\n",
    "wav_path = 'data/roller/'\n",
    "tr_path = 'data/train_wav/'\n",
    "va_path = 'data/valid_wav/'\n",
    "te_path = 'data/test_wav/'\n",
    "data_size = 66176\n",
    "tr_size = 52800\n",
    "va_size = 4576\n",
    "te_size = 8800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def wav_to_img(path, height=192, width=192):\n",
    "    signal, sr = lr.load(path, res_type='kaiser_fast')\n",
    "    hl = signal.shape[0]//(width*1.1) #this will cut away 5% from start and end\n",
    "    spec = lr.feature.melspectrogram(signal, n_mels=height, hop_length=int(hl))\n",
    "    img = lr.logamplitude(spec)**2\n",
    "    start = (img.shape[1] - width) // 2\n",
    "    return img[:, start:start+width]\n",
    "\n",
    "def process_audio(in_folder, out_folder, sub_folder):\n",
    "    \n",
    "    os.makedirs(os.path.join(out_folder, 'train_wav/' + sub_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_folder, 'valid_wav/' + sub_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_folder, 'test_wav/' + sub_folder), exist_ok=True)\n",
    "    files = shuffle(glob.glob(in_folder+'*.wav'))\n",
    "    numb_files = len(files)\n",
    "    num_train =int( 0.6*numb_files)\n",
    "    num_test = int(0.2*numb_files)\n",
    "    num_valid = int(0.2*numb_files)\n",
    "    train, test, valid = files[:num_train],files[num_train:num_train+num_test], files[num_train+num_test:]\n",
    "    start = len(in_folder)\n",
    "    for file in train:\n",
    "        img = wav_to_img(file)\n",
    "        sp.misc.imsave(os.path.join(out_folder, 'train_wav/' + sub_folder + \"/\") + file[start:] + '.jpg', img)\n",
    "    for file in test:\n",
    "        img = wav_to_img(file)\n",
    "        sp.misc.imsave(os.path.join(out_folder, 'test_wav/' + sub_folder + \"/\")+ file[start:] + '.jpg', img)\n",
    "        \n",
    "    for file in valid:\n",
    "        img = wav_to_img(file)\n",
    "        sp.misc.imsave(os.path.join(out_folder, 'valid_wav/', sub_folder+'/') + file[start:] + '.jpg', img)\n",
    "        \n",
    "    \n",
    "def process_audio_with_classes(in_folder, out_folder, labels):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    for i in range(len(labels['Sample Filename'])):\n",
    "        file = labels['Sample Filename'][i]\n",
    "        lang = labels['Language'][i]\n",
    "        os.makedirs(out_folder + lang, exist_ok=True)\n",
    "        img = mp3_to_img(in_folder+file)\n",
    "        sp.misc.imsave(out_folder + lang + '/' + file + '.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_audio('data/liners/with/', 'data', 'with')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_audio('data/liners/without/', 'data', 'without'),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "def load_dataset(base_dir):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    files = glob.glob(base_dir + '/with/*.jpg')\n",
    "    yy=[1,0]\n",
    "    for file in files:\n",
    "        x += [np.reshape(scipy.ndimage.imread(file),in_dim )]\n",
    "        y += [yy]\n",
    "    yy = [0,1]\n",
    "    files = glob.glob(base_dir + '/without/*.jpg')\n",
    "    for file in files:\n",
    "        x += [np.reshape(scipy.ndimage.imread(file),in_dim )]\n",
    "        y += [yy]\n",
    "    \n",
    "\n",
    "    x,y = shuffle(x,y)\n",
    "    x = np.array(x)/255.0\n",
    "    y = np.array(y)\n",
    "    return x,y\n",
    "\n",
    "def process_audio_in_place(in_folder):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    yy = [1,0]\n",
    "    files = shuffle(glob.glob(in_folder+'/with/*.wav'))\n",
    "    for file in files:\n",
    "        img = wav_to_img(file)\n",
    "        x += [np.reshape(img,in_dim )]\n",
    "        y += [yy]\n",
    "    yy = [0,1]\n",
    "    files = shuffle(glob.glob(in_folder+'/without/*.wav'))\n",
    "    for file in files:\n",
    "        img = wav_to_img(file)\n",
    "        x += [np.reshape(img,in_dim )]\n",
    "        y += [yy]\n",
    "    x,y = shuffle(x,y)\n",
    "    x = np.array(x)/255.0\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 192, 192, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 192, 192, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 96, 96, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 96, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 176)               90288     \n",
      "=================================================================\n",
      "Total params: 5,201,712\n",
      "Trainable params: 5,201,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=in_dim)\n",
    "m = Conv2D(16, (3, 3), activation='elu', padding='same')(i)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(32, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(64, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(128, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(256, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "hidden = Flatten()(m)\n",
    "hidden_1 = Dense(512, activation='elu')(hidden)\n",
    "m = Dropout(0.5)(hidden_1)\n",
    "o = Dense(out_dim, activation='softmax')(m)\n",
    "\n",
    "model = Model(inputs=i, outputs=o)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam(lr=1e-3), metrics=['accuracy'])\n",
    "model = load_model('speech_v9.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 2) (263, 2)\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(inputs=i, outputs=hidden)\n",
    "x, y_tr = load_dataset(\"data/train_wav\")\n",
    "\n",
    "x_tr =model2.predict(x)\n",
    "N, D = x_tr.shape\n",
    "\n",
    "x, y_val = load_dataset('data/valid_wav')\n",
    "x_val = model2.predict(x)\n",
    "\n",
    "x, y_te = load_dataset('data/test_wav')\n",
    "x_te = model2.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "print (y_tr.shape, y_val.shape)\n",
    "\n",
    "_,M = y_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9216\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,822,106\n",
      "Trainable params: 4,822,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(M)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "print(D)\n",
    "i1 =  Input(shape=[D])\n",
    "m1 = Dense(512, activation='elu')(i1)\n",
    "m1 = Dropout(0.5)(m1)\n",
    "m1 = Dense(200)(m1)\n",
    "m1 = Dropout(0.2)(m1)\n",
    "o1 = Dense(M, activation='softmax')(m1)\n",
    "model_audio = Model(inputs=i1, outputs=o1)\n",
    "model_audio.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 2) (263, 2)\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "optmiser = K.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_audio.compile(loss='categorical_crossentropy', optimizer=optmiser, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(y_tr.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_audio_wait = 'model_audio_with_without.weights.best.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 785 samples, validate on 263 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16758, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1850 - acc: 0.9962 - val_loss: 0.1676 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16758 to 0.15805, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1730 - acc: 0.9975 - val_loss: 0.1580 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15805 to 0.14928, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1646 - acc: 0.9975 - val_loss: 0.1493 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14928 to 0.14120, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1568 - acc: 0.9975 - val_loss: 0.1412 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.14120 to 0.13375, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1476 - acc: 0.9975 - val_loss: 0.1337 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13375 to 0.12685, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1398 - acc: 0.9975 - val_loss: 0.1268 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12685 to 0.12042, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1335 - acc: 0.9975 - val_loss: 0.1204 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12042 to 0.11446, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1271 - acc: 0.9975 - val_loss: 0.1145 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.11446 to 0.10892, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1220 - acc: 0.9975 - val_loss: 0.1089 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.10892 to 0.10369, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1162 - acc: 0.9975 - val_loss: 0.1037 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.10369 to 0.09886, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1110 - acc: 0.9975 - val_loss: 0.0989 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.09886 to 0.09434, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1050 - acc: 0.9975 - val_loss: 0.0943 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.09434 to 0.09007, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.1003 - acc: 0.9975 - val_loss: 0.0901 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss improved from 0.09007 to 0.08606, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0956 - acc: 0.9975 - val_loss: 0.0861 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss improved from 0.08606 to 0.08235, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0922 - acc: 0.9975 - val_loss: 0.0824 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss improved from 0.08235 to 0.07878, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0876 - acc: 0.9987 - val_loss: 0.0788 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "Epoch 00016: val_loss improved from 0.07878 to 0.07541, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0856 - acc: 0.9975 - val_loss: 0.0754 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "Epoch 00017: val_loss improved from 0.07541 to 0.07222, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0815 - acc: 0.9975 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "Epoch 00018: val_loss improved from 0.07222 to 0.06918, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0780 - acc: 0.9987 - val_loss: 0.0692 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "Epoch 00019: val_loss improved from 0.06918 to 0.06630, saving model to model_audio_with_without.weights.best.hdf5\n",
      "0s - loss: 0.0739 - acc: 0.9975 - val_loss: 0.0663 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "#model_audio = load_model(model_audio_wait)\n",
    "checkpointer = ModelCheckpoint(filepath=model_audio_wait, verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model_audio.fit(x_tr, y_tr, batch_size=100, epochs=20,\n",
    "          validation_data=(x_val, y_val), callbacks=[checkpointer], \n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_audio = load_model(model_audio_wait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/261 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.074981760554786392, 1.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_audio.evaluate(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=np.argmax(model_audio.predict(x_te), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
      " 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y_te1 = process_audio_in_place('data/roller_test')\n",
    "\n",
    "#x, y_te1 = process_audio_in_place('data/liners')\n",
    "x_te1 = model2.predict(x)\n",
    "print(x_te1.shape)\n",
    "\n",
    "model_audio.evaluate(x_te1,y_te1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "t =model_audio.predict(x_te1)\n",
    "y = np.argmax(t,axis=1)\n",
    "print(y)\n",
    "\n",
    "y = np.argmax(y_te1,axis=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
